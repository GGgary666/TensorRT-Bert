{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01.测试 Bert Model\n",
    "1. 初始化tokenizer和Bert model，设置用于测试的text\n",
    "2. 基于pytorch执行bert推理，输出概率最高的10个词\n",
    "3. 保存输出信息，用来和之后转换过的模型进行对比\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import time\n",
    "import numpy as np\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "BERT_PATH = '../bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_PATH)\n",
    "model = BertForMaskedLM.from_pretrained(BERT_PATH, return_dict = True)\n",
    "text = \"The capital of France, \" + tokenizer.mask_token + \", contains the Eiffel Tower.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input ids: \n",
      " tensor([[  101,  1996,  3007,  1997,  2605,  1010,   103,  1010,  3397,  1996,\n",
      "          1041, 13355,  2884,  3578,  1012,   102]])\n",
      "output shape:  torch.Size([1, 16, 30522])\n",
      "model test topk10 output:\n",
      "The capital of France, paris, contains the Eiffel Tower.\n",
      "The capital of France, lyon, contains the Eiffel Tower.\n",
      "The capital of France, lille, contains the Eiffel Tower.\n",
      "The capital of France, toulouse, contains the Eiffel Tower.\n",
      "The capital of France, marseille, contains the Eiffel Tower.\n",
      "The capital of France, orleans, contains the Eiffel Tower.\n",
      "The capital of France, strasbourg, contains the Eiffel Tower.\n",
      "The capital of France, nice, contains the Eiffel Tower.\n",
      "The capital of France, cannes, contains the Eiffel Tower.\n",
      "The capital of France, versailles, contains the Eiffel Tower.\n",
      "****************************************\n",
      "pytorch with bin model running time: 0.02549151040002471\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer.encode_plus(text, return_tensors = \"pt\")\n",
    "mask_index = torch.where(encoded_input[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
    "print(\"input ids: \\n\",encoded_input[\"input_ids\"])\n",
    "\n",
    "# warm up\n",
    "for i in range(5):\n",
    "    output = model(**encoded_input)\n",
    "start_time = time.perf_counter()\n",
    "# 计算平均推理时间\n",
    "for i in range(10):\n",
    "    output = model(**encoded_input)\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "print(\"output shape: \", output[0].shape)\n",
    "logits = output.logits\n",
    "softmax = F.softmax(logits, dim = -1)\n",
    "mask_word = softmax[0, mask_index, :]\n",
    "top_10 = torch.topk(mask_word, 10, dim = 1)[1][0]\n",
    "print(\"model test topk10 output:\")\n",
    "for token in top_10:\n",
    "    word = tokenizer.decode([token])\n",
    "    new_sentence = text.replace(tokenizer.mask_token, word)\n",
    "    print(new_sentence)\n",
    "print('*' * 40)\n",
    "print(\"pytorch with bin model running time:\", (end_time-start_time)*100, \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving inputs and output to case_data.npz ...\n",
      "position id:  tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]],\n",
      "       dtype=torch.int32)\n",
      "input_id shape:  (1, 16)\n",
      "saved input ids: \n",
      " [[  101  1996  3007  1997  2605  1010   103  1010  3397  1996  1041 13355\n",
      "   2884  3578  1012   102]]\n"
     ]
    }
   ],
   "source": [
    "# save inputs and output\n",
    "print(\"Saving inputs and output to case_data.npz ...\")\n",
    "position_ids = torch.arange(0, encoded_input['input_ids'].shape[1]).int().view(1, -1)\n",
    "print(\"position id: \",position_ids)\n",
    "input_ids=encoded_input['input_ids'].int().detach().numpy()\n",
    "token_type_ids=encoded_input['token_type_ids'].int().detach().numpy()\n",
    "print(\"input_id shape: \",input_ids.shape)\n",
    "# save data\n",
    "npz_file = BERT_PATH + '/case_data.npz'\n",
    "np.savez(npz_file,\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            logits=output[0].detach().numpy())\n",
    "\n",
    "data = np.load(npz_file)\n",
    "print(\"saved input ids: \\n\", data['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input[\"attention_mask\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02. 将模型转换为ONNX格式\n",
    "使用torch.onnx.export() 进行转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported at  bert-base-uncased/model.onnx\n"
     ]
    }
   ],
   "source": [
    "# convert model to onnx\n",
    "model.eval()\n",
    "export_model_path = BERT_PATH + \"/model.onnx\"\n",
    "opset_version = 16\n",
    "symbolic_names = {0: 'batch_size', 1: 'max_seq_len'}\n",
    "torch.onnx.export(  model,                                            \n",
    "                    args=tuple(encoded_input.values()),               # model input (or a tuple for multiple inputs)\n",
    "                    f=export_model_path,                              # where to save the model (can be a file or file-like object)\n",
    "                    opset_version=opset_version,                      # the ONNX version to export the model to\n",
    "                    do_constant_folding=False,                        # whether to execute constant folding for optimization\n",
    "                    input_names=['input_ids',                         # the model's input names\n",
    "                                'attention_mask',\n",
    "                                'token_type_ids'],\n",
    "                    output_names=['logits'],                          # the model's output names\n",
    "                    dynamic_axes={'input_ids': symbolic_names,        # variable length axes\n",
    "                                'attention_mask' : symbolic_names,\n",
    "                                'token_type_ids' : symbolic_names,\n",
    "                                'logits' : symbolic_names})\n",
    "print(\"Model exported at \", export_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03. 使用onnxruntime进行onnx推理\n",
    "与pytorch和tensorrt的推理时间相对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnxruntime version: 1.16.3\n",
      "onnxruntime device: GPU\n"
     ]
    }
   ],
   "source": [
    "# 检查设备是否为GPU\n",
    "print(\"onnxruntime version:\", ort.__version__)\n",
    "print(\"onnxruntime device:\", ort.get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert SUCCESS!!!!!!\n",
      "****************************************\n",
      "pytorch with bin model running time: 0.018559776899928694\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "session = ort.InferenceSession(export_model_path)\n",
    "# 执行推理\n",
    "# warmup\n",
    "for i in range(5):\n",
    "    outputs = session.run(['logits'], {'input_ids': encoded_input['input_ids'].numpy(),\n",
    "                                    'attention_mask': encoded_input['attention_mask'].numpy(),\n",
    "                                   'token_type_ids': encoded_input['token_type_ids'].numpy()})[0]\n",
    "start_time = time.perf_counter()\n",
    "for i in range(10):\n",
    "    outputs = session.run(['logits'], {'input_ids': encoded_input['input_ids'].numpy(),\n",
    "                                    'attention_mask': encoded_input['attention_mask'].numpy(),\n",
    "                                   'token_type_ids': encoded_input['token_type_ids'].numpy()})[0]\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "# 检查转换后的模型的精度损失情况\n",
    "required_precission = 1e-4\n",
    "precesion_loss = np.abs(outputs - data['logits'])\n",
    "boolean_mask = precesion_loss > required_precission\n",
    "if(len(np.where(boolean_mask)[0]) > 0):\n",
    "    print(\"Convert ERROR!\")\n",
    "else:\n",
    "    print(\"Convert SUCCESS!!!!!!\")\n",
    "print('*' * 40)\n",
    "print(\"pytorch with bin model running time:\", (end_time-start_time)*100, \"ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
