{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import time\n",
    "import onnx\n",
    "import tensorrt as trt\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01. 简化ONNX model\n",
    "使用onnxsim库，进行常量折叠。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplifying\u001b[33m...\u001b[0m\n",
      "Finish! Here is the difference:\n",
      "┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOriginal Model\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSimplified Model\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
      "│ Add        │ 177            │ 177              │\n",
      "│ Cast       │ 1              │ 1                │\n",
      "│ Concat     │ 48             │ \u001b[1;32m0               \u001b[0m │\n",
      "│ Constant   │ 596            │ \u001b[1;32m213             \u001b[0m │\n",
      "│ Div        │ 51             │ 51               │\n",
      "│ Erf        │ 13             │ 13               │\n",
      "│ Gather     │ 100            │ \u001b[1;32m2               \u001b[0m │\n",
      "│ MatMul     │ 98             │ 98               │\n",
      "│ Mul        │ 53             │ 53               │\n",
      "│ Pow        │ 26             │ 26               │\n",
      "│ ReduceMean │ 52             │ 52               │\n",
      "│ Reshape    │ 48             │ 48               │\n",
      "│ Shape      │ 97             │ \u001b[1;32m0               \u001b[0m │\n",
      "│ Slice      │ 1              │ \u001b[1;32m0               \u001b[0m │\n",
      "│ Softmax    │ 12             │ 12               │\n",
      "│ Sqrt       │ 26             │ 26               │\n",
      "│ Sub        │ 27             │ 27               │\n",
      "│ Transpose  │ 122            │ \u001b[1;32m48              \u001b[0m │\n",
      "│ Unsqueeze  │ 186            │ \u001b[1;32m1               \u001b[0m │\n",
      "│ Model Size │ 418.1MiB       │ 505.9MiB         │\n",
      "└────────────┴────────────────┴──────────────────┘\n"
     ]
    }
   ],
   "source": [
    "!onnxsim ../bert-base-uncased/model.onnx ../bert-base-uncased/model-sim.onnx --overwrite-input-shape input_ids:1,16 token_type_ids:1,16 attention_mask:1,16 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02. 验证简化后的ONNX模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取、设置输入数据\n",
    "BERT_PATH = '../bert-base-uncased'\n",
    "npz_file = BERT_PATH + '/case_data.npz'\n",
    "data = np.load(npz_file)\n",
    "input_ids = data[\"input_ids\"].astype(np.int64)\n",
    "attention_mask = np.ones((1, 16), dtype = np.int64)\n",
    "token_type_ids = np.zeros((1, 16), dtype = np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplify SUCCESS!!!!!!\n",
      "****************************************\n",
      "onnxruntime with simplified onnx model running time: 0.017777415999807998\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "session = ort.InferenceSession(\"../bert-base-uncased/model-sim.onnx\")\n",
    "# 执行推理\n",
    "# warmup\n",
    "for i in range(5):\n",
    "    outputs1 = session.run(['logits'], {'input_ids': input_ids,\n",
    "                                    'attention_mask': attention_mask,\n",
    "                                   'token_type_ids': token_type_ids})[0]\n",
    "start_time = time.perf_counter()\n",
    "for i in range(10):\n",
    "    outputs1 = session.run(['logits'], {'input_ids': input_ids,\n",
    "                                    'attention_mask': attention_mask,\n",
    "                                   'token_type_ids': token_type_ids})[0]\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "# 检查转换后的模型的精度损失情况\n",
    "required_precission = 1e-4\n",
    "precesion_loss = np.abs(outputs1 - data['logits'])\n",
    "boolean_mask = precesion_loss > required_precission\n",
    "if(len(np.where(boolean_mask)[0]) > 0):\n",
    "    print(\"Simplify ERROR!\")\n",
    "else:\n",
    "    print(\"Simplify SUCCESS!!!!!!\")\n",
    "print('*' * 40)\n",
    "print(\"onnxruntime with simplified onnx model running time:\", (end_time-start_time)*100, \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03. 创建TensorRT Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_384533/3082242190.py:8: DeprecationWarning: Use set_memory_pool_limit instead.\n",
      "  config.max_workspace_size = 512*1024*1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/02/2024-19:33:36] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "network.num_layers 1162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_384533/3082242190.py:18: DeprecationWarning: Use build_serialized_network instead.\n",
      "  engine = builder.build_engine(network, config=config)\n"
     ]
    }
   ],
   "source": [
    "# 创建engine\n",
    "model_file = \"../bert-base-uncased/model-sim.onnx\"\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "builder = trt.Builder(TRT_LOGGER)\n",
    "\n",
    "config = builder.create_builder_config()\n",
    "\n",
    "config.max_workspace_size = 512*1024*1024\n",
    "\n",
    "explicit_batch = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "network = builder.create_network(explicit_batch)\n",
    "with trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "    with open(model_file, 'rb') as model:\n",
    "        parsed = parser.parse(model.read())\n",
    "        print(\"network.num_layers\", network.num_layers)\n",
    "        #last_layer = network.get_layer(network.num_layers - 1)\n",
    "        #network.mark_output(last_layer.get_output(0))\n",
    "        engine = builder.build_engine(network, config=config)\n",
    "        \n",
    "        \n",
    "# save the paln model\n",
    "BERT_PATH = '../bert-base-uncased'\n",
    "plan_path = BERT_PATH +'/model.plan'\n",
    "with open(plan_path, 'wb') as f:\n",
    "    f.write(engine.serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8601] # trtexec --onnx=bert-base-uncased/model-sim.onnx --saveEngine=bert-base-uncased/model.trt --explicitBatch\n",
      "[02/02/2024-13:05:32] [W] --explicitBatch flag has been deprecated and has no effect!\n",
      "[02/02/2024-13:05:32] [W] Explicit batch dim is automatically enabled if input model is ONNX or if dynamic shapes are provided when the engine is built.\n",
      "[02/02/2024-13:05:32] [I] === Model Options ===\n",
      "[02/02/2024-13:05:32] [I] Format: ONNX\n",
      "[02/02/2024-13:05:32] [I] Model: bert-base-uncased/model-sim.onnx\n",
      "[02/02/2024-13:05:32] [I] Output:\n",
      "[02/02/2024-13:05:32] [I] === Build Options ===\n",
      "[02/02/2024-13:05:32] [I] Max batch: explicit batch\n",
      "[02/02/2024-13:05:32] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default\n",
      "[02/02/2024-13:05:32] [I] minTiming: 1\n",
      "[02/02/2024-13:05:32] [I] avgTiming: 8\n",
      "[02/02/2024-13:05:32] [I] Precision: FP32\n",
      "[02/02/2024-13:05:32] [I] LayerPrecisions: \n",
      "[02/02/2024-13:05:32] [I] Layer Device Types: \n",
      "[02/02/2024-13:05:32] [I] Calibration: \n",
      "[02/02/2024-13:05:32] [I] Refit: Disabled\n",
      "[02/02/2024-13:05:32] [I] Version Compatible: Disabled\n",
      "[02/02/2024-13:05:32] [I] TensorRT runtime: full\n",
      "[02/02/2024-13:05:32] [I] Lean DLL Path: \n",
      "[02/02/2024-13:05:32] [I] Tempfile Controls: { in_memory: allow, temporary: allow }\n",
      "[02/02/2024-13:05:32] [I] Exclude Lean Runtime: Disabled\n",
      "[02/02/2024-13:05:32] [I] Sparsity: Disabled\n",
      "[02/02/2024-13:05:32] [I] Safe mode: Disabled\n",
      "[02/02/2024-13:05:32] [I] Build DLA standalone loadable: Disabled\n",
      "[02/02/2024-13:05:32] [I] Allow GPU fallback for DLA: Disabled\n",
      "[02/02/2024-13:05:32] [I] DirectIO mode: Disabled\n",
      "[02/02/2024-13:05:32] [I] Restricted mode: Disabled\n",
      "[02/02/2024-13:05:32] [I] Skip inference: Disabled\n",
      "[02/02/2024-13:05:32] [I] Save engine: bert-base-uncased/model.trt\n",
      "[02/02/2024-13:05:32] [I] Load engine: \n",
      "[02/02/2024-13:05:32] [I] Profiling verbosity: 0\n",
      "[02/02/2024-13:05:32] [I] Tactic sources: Using default tactic sources\n",
      "[02/02/2024-13:05:32] [I] timingCacheMode: local\n",
      "[02/02/2024-13:05:32] [I] timingCacheFile: \n",
      "[02/02/2024-13:05:32] [I] Heuristic: Disabled\n",
      "[02/02/2024-13:05:32] [I] Preview Features: Use default preview flags.\n",
      "[02/02/2024-13:05:32] [I] MaxAuxStreams: -1\n",
      "[02/02/2024-13:05:32] [I] BuilderOptimizationLevel: -1\n",
      "[02/02/2024-13:05:32] [I] Input(s)s format: fp32:CHW\n",
      "[02/02/2024-13:05:32] [I] Output(s)s format: fp32:CHW\n",
      "[02/02/2024-13:05:32] [I] Input build shapes: model\n",
      "[02/02/2024-13:05:32] [I] Input calibration shapes: model\n",
      "[02/02/2024-13:05:32] [I] === System Options ===\n",
      "[02/02/2024-13:05:32] [I] Device: 0\n",
      "[02/02/2024-13:05:32] [I] DLACore: \n",
      "[02/02/2024-13:05:32] [I] Plugins:\n",
      "[02/02/2024-13:05:32] [I] setPluginsToSerialize:\n",
      "[02/02/2024-13:05:32] [I] dynamicPlugins:\n",
      "[02/02/2024-13:05:32] [I] ignoreParsedPluginLibs: 0\n",
      "[02/02/2024-13:05:32] [I] \n",
      "[02/02/2024-13:05:32] [I] === Inference Options ===\n",
      "[02/02/2024-13:05:32] [I] Batch: Explicit\n",
      "[02/02/2024-13:05:32] [I] Input inference shapes: model\n",
      "[02/02/2024-13:05:32] [I] Iterations: 10\n",
      "[02/02/2024-13:05:32] [I] Duration: 3s (+ 200ms warm up)\n",
      "[02/02/2024-13:05:32] [I] Sleep time: 0ms\n",
      "[02/02/2024-13:05:32] [I] Idle time: 0ms\n",
      "[02/02/2024-13:05:32] [I] Inference Streams: 1\n",
      "[02/02/2024-13:05:32] [I] ExposeDMA: Disabled\n",
      "[02/02/2024-13:05:32] [I] Data transfers: Enabled\n",
      "[02/02/2024-13:05:32] [I] Spin-wait: Disabled\n",
      "[02/02/2024-13:05:32] [I] Multithreading: Disabled\n",
      "[02/02/2024-13:05:32] [I] CUDA Graph: Disabled\n",
      "[02/02/2024-13:05:32] [I] Separate profiling: Disabled\n",
      "[02/02/2024-13:05:32] [I] Time Deserialize: Disabled\n",
      "[02/02/2024-13:05:32] [I] Time Refit: Disabled\n",
      "[02/02/2024-13:05:32] [I] NVTX verbosity: 0\n",
      "[02/02/2024-13:05:32] [I] Persistent Cache Ratio: 0\n",
      "[02/02/2024-13:05:32] [I] Inputs:\n",
      "[02/02/2024-13:05:32] [I] === Reporting Options ===\n",
      "[02/02/2024-13:05:32] [I] Verbose: Disabled\n",
      "[02/02/2024-13:05:32] [I] Averages: 10 inferences\n",
      "[02/02/2024-13:05:32] [I] Percentiles: 90,95,99\n",
      "[02/02/2024-13:05:32] [I] Dump refittable layers:Disabled\n",
      "[02/02/2024-13:05:32] [I] Dump output: Disabled\n",
      "[02/02/2024-13:05:32] [I] Profile: Disabled\n",
      "[02/02/2024-13:05:32] [I] Export timing to JSON file: \n",
      "[02/02/2024-13:05:32] [I] Export output to JSON file: \n",
      "[02/02/2024-13:05:32] [I] Export profile to JSON file: \n",
      "[02/02/2024-13:05:32] [I] \n",
      "[02/02/2024-13:05:32] [I] === Device Information ===\n",
      "[02/02/2024-13:05:32] [I] Selected Device: NVIDIA GeForce RTX 4060 Ti\n",
      "[02/02/2024-13:05:32] [I] Compute Capability: 8.9\n",
      "[02/02/2024-13:05:32] [I] SMs: 34\n",
      "[02/02/2024-13:05:32] [I] Device Global Memory: 16074 MiB\n",
      "[02/02/2024-13:05:32] [I] Shared Memory per SM: 100 KiB\n",
      "[02/02/2024-13:05:32] [I] Memory Bus Width: 128 bits (ECC disabled)\n",
      "[02/02/2024-13:05:32] [I] Application Compute Clock Rate: 2.565 GHz\n",
      "[02/02/2024-13:05:32] [I] Application Memory Clock Rate: 9.001 GHz\n",
      "[02/02/2024-13:05:32] [I] \n",
      "[02/02/2024-13:05:32] [I] Note: The application clock rates do not reflect the actual clock rates that the GPU is currently running at.\n",
      "[02/02/2024-13:05:32] [I] \n",
      "[02/02/2024-13:05:32] [I] TensorRT version: 8.6.1\n",
      "[02/02/2024-13:05:32] [I] Loading standard plugins\n",
      "[02/02/2024-13:05:32] [I] [TRT] [MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 19, GPU 1034 (MiB)\n",
      "[02/02/2024-13:05:36] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +1445, GPU +268, now: CPU 1540, GPU 1304 (MiB)\n",
      "[02/02/2024-13:05:36] [I] Start parsing network model.\n",
      "[02/02/2024-13:05:36] [I] [TRT] ----------------------------------------------------------------\n",
      "[02/02/2024-13:05:36] [I] [TRT] Input filename:   bert-base-uncased/model-sim.onnx\n",
      "[02/02/2024-13:05:36] [I] [TRT] ONNX IR version:  0.0.8\n",
      "[02/02/2024-13:05:36] [I] [TRT] Opset version:    16\n",
      "[02/02/2024-13:05:36] [I] [TRT] Producer name:    pytorch\n",
      "[02/02/2024-13:05:36] [I] [TRT] Producer version: 2.2.0\n",
      "[02/02/2024-13:05:36] [I] [TRT] Domain:           \n",
      "[02/02/2024-13:05:36] [I] [TRT] Model version:    0\n",
      "[02/02/2024-13:05:36] [I] [TRT] Doc string:       \n",
      "[02/02/2024-13:05:36] [I] [TRT] ----------------------------------------------------------------\n",
      "[02/02/2024-13:05:37] [W] [TRT] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[02/02/2024-13:05:37] [I] Finished parsing network model. Parse time: 0.685949\n",
      "[02/02/2024-13:05:37] [I] [TRT] Graph optimization time: 0.0390756 seconds.\n",
      "[02/02/2024-13:05:37] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[02/02/2024-13:05:55] [I] [TRT] Detected 3 inputs and 1 output network tensors.\n",
      "[02/02/2024-13:05:55] [I] [TRT] Total Host Persistent Memory: 32\n",
      "[02/02/2024-13:05:55] [I] [TRT] Total Device Persistent Memory: 0\n",
      "[02/02/2024-13:05:55] [I] [TRT] Total Scratch Memory: 664064\n",
      "[02/02/2024-13:05:55] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 509 MiB\n",
      "[02/02/2024-13:05:55] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 1 steps to complete.\n",
      "[02/02/2024-13:05:55] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 0.006457ms to assign 1 blocks to 1 nodes requiring 664064 bytes.\n",
      "[02/02/2024-13:05:55] [I] [TRT] Total Activation Memory: 664064\n",
      "[02/02/2024-13:05:56] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +512, now: CPU 0, GPU 512 (MiB)\n",
      "[02/02/2024-13:05:56] [I] Engine built in 24.4592 sec.\n",
      "[02/02/2024-13:05:57] [I] [TRT] Loaded engine size: 506 MiB\n",
      "[02/02/2024-13:05:57] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +505, now: CPU 0, GPU 505 (MiB)\n",
      "[02/02/2024-13:05:57] [I] Engine deserialized in 0.284354 sec.\n",
      "[02/02/2024-13:05:57] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +1, now: CPU 0, GPU 506 (MiB)\n",
      "[02/02/2024-13:05:57] [I] Setting persistentCacheLimit to 0 bytes.\n",
      "[02/02/2024-13:05:57] [I] Using random values for input input_ids\n",
      "[02/02/2024-13:05:57] [I] Input binding for input_ids with dimensions 1x16 is created.\n",
      "[02/02/2024-13:05:57] [I] Using random values for input attention_mask\n",
      "[02/02/2024-13:05:57] [I] Input binding for attention_mask with dimensions 1x16 is created.\n",
      "[02/02/2024-13:05:57] [I] Using random values for input token_type_ids\n",
      "[02/02/2024-13:05:57] [I] Input binding for token_type_ids with dimensions 1x16 is created.\n",
      "[02/02/2024-13:05:57] [I] Output binding for logits with dimensions 1x16x30522 is created.\n",
      "[02/02/2024-13:05:57] [I] Starting inference\n",
      "[02/02/2024-13:06:00] [I] Warmup completed 89 queries over 200 ms\n",
      "[02/02/2024-13:06:00] [I] Timing trace has 1362 queries over 3.00613 s\n",
      "[02/02/2024-13:06:00] [I] \n",
      "[02/02/2024-13:06:00] [I] === Trace details ===\n",
      "[02/02/2024-13:06:00] [I] Trace averages of 10 runs:\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.22853 ms - Host latency: 2.41094 ms (enqueue 0.586336 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.2829 ms - Host latency: 2.45987 ms (enqueue 0.720136 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.13637 ms - Host latency: 2.3129 ms (enqueue 0.6231 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21136 ms - Host latency: 2.39043 ms (enqueue 0.75672 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21998 ms - Host latency: 2.43589 ms (enqueue 0.734338 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.19361 ms - Host latency: 2.36693 ms (enqueue 0.678088 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.22338 ms - Host latency: 2.40355 ms (enqueue 0.73876 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.28179 ms - Host latency: 2.46013 ms (enqueue 0.69592 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17365 ms - Host latency: 2.35204 ms (enqueue 0.740778 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.24759 ms - Host latency: 2.42375 ms (enqueue 0.704315 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.32014 ms - Host latency: 2.5006 ms (enqueue 0.781061 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17437 ms - Host latency: 2.37447 ms (enqueue 0.734659 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.23723 ms - Host latency: 2.41367 ms (enqueue 0.701706 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.28332 ms - Host latency: 2.45455 ms (enqueue 0.592255 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17651 ms - Host latency: 2.34794 ms (enqueue 0.649884 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20099 ms - Host latency: 2.39803 ms (enqueue 0.663196 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.23487 ms - Host latency: 2.41335 ms (enqueue 0.695416 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.15954 ms - Host latency: 2.33643 ms (enqueue 0.746429 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21359 ms - Host latency: 2.42303 ms (enqueue 0.692944 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.27914 ms - Host latency: 2.4514 ms (enqueue 0.652405 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.12941 ms - Host latency: 2.30851 ms (enqueue 0.39007 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21285 ms - Host latency: 2.38579 ms (enqueue 0.327277 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17969 ms - Host latency: 2.34836 ms (enqueue 0.378314 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.23571 ms - Host latency: 2.40016 ms (enqueue 0.222876 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18879 ms - Host latency: 2.35286 ms (enqueue 0.262231 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18082 ms - Host latency: 2.34791 ms (enqueue 0.380676 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20909 ms - Host latency: 2.37885 ms (enqueue 0.548114 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.16677 ms - Host latency: 2.33036 ms (enqueue 0.320453 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18492 ms - Host latency: 2.3545 ms (enqueue 0.634058 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17118 ms - Host latency: 2.34364 ms (enqueue 0.629175 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.24861 ms - Host latency: 2.42137 ms (enqueue 0.644641 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17826 ms - Host latency: 2.3504 ms (enqueue 0.674048 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20518 ms - Host latency: 2.38011 ms (enqueue 0.692413 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.2399 ms - Host latency: 2.41335 ms (enqueue 0.665417 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18585 ms - Host latency: 2.39392 ms (enqueue 0.618805 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21503 ms - Host latency: 2.3795 ms (enqueue 0.34422 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.23058 ms - Host latency: 2.46426 ms (enqueue 0.603632 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18572 ms - Host latency: 2.35602 ms (enqueue 0.661157 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18962 ms - Host latency: 2.44231 ms (enqueue 0.597986 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.24384 ms - Host latency: 2.4182 ms (enqueue 0.492236 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17313 ms - Host latency: 2.35718 ms (enqueue 0.547034 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.19945 ms - Host latency: 2.37567 ms (enqueue 0.728394 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20366 ms - Host latency: 2.38842 ms (enqueue 0.754541 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.1639 ms - Host latency: 2.34023 ms (enqueue 0.720276 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.19907 ms - Host latency: 2.37306 ms (enqueue 0.679785 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.25496 ms - Host latency: 2.43036 ms (enqueue 0.71991 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.1653 ms - Host latency: 2.34224 ms (enqueue 0.654175 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.28433 ms - Host latency: 2.48632 ms (enqueue 0.613135 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.25422 ms - Host latency: 2.42512 ms (enqueue 0.634595 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.1627 ms - Host latency: 2.38567 ms (enqueue 0.691284 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.22422 ms - Host latency: 2.3978 ms (enqueue 0.681067 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18246 ms - Host latency: 2.3551 ms (enqueue 0.737451 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18678 ms - Host latency: 2.36085 ms (enqueue 0.700293 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.2309 ms - Host latency: 2.40273 ms (enqueue 0.687244 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.1939 ms - Host latency: 2.36589 ms (enqueue 0.682983 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.1849 ms - Host latency: 2.35665 ms (enqueue 0.640588 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20557 ms - Host latency: 2.40814 ms (enqueue 0.690649 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.19557 ms - Host latency: 2.37277 ms (enqueue 0.671826 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18929 ms - Host latency: 2.39972 ms (enqueue 0.686462 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.25649 ms - Host latency: 2.42864 ms (enqueue 0.650879 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17297 ms - Host latency: 2.34592 ms (enqueue 0.66366 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.22762 ms - Host latency: 2.40157 ms (enqueue 0.68866 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.26277 ms - Host latency: 2.4761 ms (enqueue 0.667993 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.1651 ms - Host latency: 2.33254 ms (enqueue 0.531006 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.25006 ms - Host latency: 2.42709 ms (enqueue 0.645215 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.23591 ms - Host latency: 2.44139 ms (enqueue 0.54353 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18702 ms - Host latency: 2.36449 ms (enqueue 0.665039 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.16669 ms - Host latency: 2.3573 ms (enqueue 0.4479 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.25352 ms - Host latency: 2.42657 ms (enqueue 0.530225 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.14072 ms - Host latency: 2.31387 ms (enqueue 0.640271 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20887 ms - Host latency: 2.45624 ms (enqueue 0.801306 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21603 ms - Host latency: 2.39735 ms (enqueue 0.741895 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.16801 ms - Host latency: 2.35758 ms (enqueue 0.402319 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20116 ms - Host latency: 2.36769 ms (enqueue 0.443982 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20703 ms - Host latency: 2.43057 ms (enqueue 0.440149 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.19373 ms - Host latency: 2.37385 ms (enqueue 0.592114 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20662 ms - Host latency: 2.38062 ms (enqueue 0.422168 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18929 ms - Host latency: 2.35677 ms (enqueue 0.324292 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.12858 ms - Host latency: 2.28925 ms (enqueue 0.168359 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.19937 ms - Host latency: 2.36097 ms (enqueue 0.248633 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.15195 ms - Host latency: 2.32847 ms (enqueue 0.52251 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17885 ms - Host latency: 2.34467 ms (enqueue 0.437256 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.24469 ms - Host latency: 2.41315 ms (enqueue 0.513818 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.13184 ms - Host latency: 2.30193 ms (enqueue 0.635059 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.22476 ms - Host latency: 2.39463 ms (enqueue 0.632471 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.26194 ms - Host latency: 2.5146 ms (enqueue 0.63938 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17175 ms - Host latency: 2.34399 ms (enqueue 0.658716 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20942 ms - Host latency: 2.47114 ms (enqueue 0.778491 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21616 ms - Host latency: 2.39783 ms (enqueue 0.596338 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.1252 ms - Host latency: 2.3073 ms (enqueue 0.567627 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20569 ms - Host latency: 2.37808 ms (enqueue 0.434839 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18909 ms - Host latency: 2.3575 ms (enqueue 0.594775 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18413 ms - Host latency: 2.38032 ms (enqueue 0.634814 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.26125 ms - Host latency: 2.43289 ms (enqueue 0.655981 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.19653 ms - Host latency: 2.38064 ms (enqueue 0.62627 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20635 ms - Host latency: 2.37617 ms (enqueue 0.649878 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21355 ms - Host latency: 2.40034 ms (enqueue 0.618555 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.26321 ms - Host latency: 2.43354 ms (enqueue 0.605054 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17314 ms - Host latency: 2.34272 ms (enqueue 0.654834 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.23301 ms - Host latency: 2.39905 ms (enqueue 0.43147 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21267 ms - Host latency: 2.37673 ms (enqueue 0.427051 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.13328 ms - Host latency: 2.29724 ms (enqueue 0.248267 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20906 ms - Host latency: 2.37507 ms (enqueue 0.331616 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.14387 ms - Host latency: 2.3386 ms (enqueue 0.430688 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.22371 ms - Host latency: 2.3999 ms (enqueue 0.653882 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.26194 ms - Host latency: 2.43025 ms (enqueue 0.548853 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.15469 ms - Host latency: 2.32393 ms (enqueue 0.52439 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21699 ms - Host latency: 2.4072 ms (enqueue 0.643384 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.24517 ms - Host latency: 2.41895 ms (enqueue 0.680054 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.13586 ms - Host latency: 2.30842 ms (enqueue 0.647192 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.22661 ms - Host latency: 2.39636 ms (enqueue 0.540503 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.34346 ms - Host latency: 2.5134 ms (enqueue 0.459595 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.13032 ms - Host latency: 2.35818 ms (enqueue 0.621362 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20664 ms - Host latency: 2.3769 ms (enqueue 0.580249 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21753 ms - Host latency: 2.38826 ms (enqueue 0.592358 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18115 ms - Host latency: 2.3501 ms (enqueue 0.617554 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.1782 ms - Host latency: 2.36582 ms (enqueue 0.540088 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21553 ms - Host latency: 2.3927 ms (enqueue 0.677148 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.14443 ms - Host latency: 2.31707 ms (enqueue 0.642163 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21648 ms - Host latency: 2.42805 ms (enqueue 0.603687 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21033 ms - Host latency: 2.37976 ms (enqueue 0.589429 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.16008 ms - Host latency: 2.33853 ms (enqueue 0.670825 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.23396 ms - Host latency: 2.40398 ms (enqueue 0.582642 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.15686 ms - Host latency: 2.32275 ms (enqueue 0.387085 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17585 ms - Host latency: 2.34417 ms (enqueue 0.553809 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.24316 ms - Host latency: 2.41995 ms (enqueue 0.721509 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.15613 ms - Host latency: 2.32949 ms (enqueue 0.697827 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.1884 ms - Host latency: 2.3606 ms (enqueue 0.668823 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.23374 ms - Host latency: 2.40718 ms (enqueue 0.672949 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.15151 ms - Host latency: 2.32485 ms (enqueue 0.702393 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21355 ms - Host latency: 2.38901 ms (enqueue 0.731299 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.23176 ms - Host latency: 2.40898 ms (enqueue 0.680127 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.16987 ms - Host latency: 2.34687 ms (enqueue 0.718799 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.22839 ms - Host latency: 2.40422 ms (enqueue 0.618066 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18457 ms - Host latency: 2.35193 ms (enqueue 0.437183 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17297 ms - Host latency: 2.33674 ms (enqueue 0.228491 ms)\n",
      "[02/02/2024-13:06:00] [I] \n",
      "[02/02/2024-13:06:00] [I] === Performance summary ===\n",
      "[02/02/2024-13:06:00] [I] Throughput: 453.074 qps\n",
      "[02/02/2024-13:06:00] [I] Latency: min = 2.12183 ms, max = 3.22327 ms, mean = 2.38405 ms, median = 2.41956 ms, percentile(90%) = 2.67151 ms, percentile(95%) = 2.71201 ms, percentile(99%) = 2.88208 ms\n",
      "[02/02/2024-13:06:00] [I] Enqueue Time: min = 0.139404 ms, max = 1.23145 ms, mean = 0.592865 ms, median = 0.631531 ms, percentile(90%) = 0.755676 ms, percentile(95%) = 0.852783 ms, percentile(99%) = 1.07861 ms\n",
      "[02/02/2024-13:06:00] [I] H2D Latency: min = 0.00512695 ms, max = 0.504395 ms, mean = 0.0272168 ms, median = 0.0170898 ms, percentile(90%) = 0.034668 ms, percentile(95%) = 0.0465088 ms, percentile(99%) = 0.346436 ms\n",
      "[02/02/2024-13:06:00] [I] GPU Compute Time: min = 1.95874 ms, max = 2.73407 ms, mean = 2.20366 ms, median = 2.24164 ms, percentile(90%) = 2.47607 ms, percentile(95%) = 2.52515 ms, percentile(99%) = 2.59993 ms\n",
      "[02/02/2024-13:06:00] [I] D2H Latency: min = 0.150818 ms, max = 0.164307 ms, mean = 0.153176 ms, median = 0.152893 ms, percentile(90%) = 0.154541 ms, percentile(95%) = 0.155273 ms, percentile(99%) = 0.156372 ms\n",
      "[02/02/2024-13:06:00] [I] Total Host Walltime: 3.00613 s\n",
      "[02/02/2024-13:06:00] [I] Total GPU Compute Time: 3.00138 s\n",
      "[02/02/2024-13:06:00] [W] * GPU compute time is unstable, with coefficient of variance = 9.6148%.\n",
      "[02/02/2024-13:06:00] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.\n",
      "[02/02/2024-13:06:00] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[02/02/2024-13:06:00] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8601] # trtexec --onnx=bert-base-uncased/model-sim.onnx --saveEngine=bert-base-uncased/model.trt --explicitBatch\n"
     ]
    }
   ],
   "source": [
    "# 也可以使用命令行的方式，执行trtexec进行trt engine的创建\n",
    "# !trtexec --onnx=../bert-base-uncased/model-sim.onnx --saveEngine=../bert-base-uncased/model.trt  --explicitBatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04. 利用TensorRT执行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取engine执行推理\n",
    "BERT_PATH = 'bert-base-uncased'\n",
    "plan_path = BERT_PATH +'/model.plan'\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "runtime = trt.Runtime(TRT_LOGGER)\n",
    "with open(plan_path, 'rb') as f:\n",
    "    engine_bytes = f.read()\n",
    "    engine = runtime.deserialize_cuda_engine(engine_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建执行上下文\n",
    "bert_context = engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新设置输入数据的格式（因为trt将onnx模型中的int64转换为int32）\n",
    "input_ids = input_ids.astype(np.int32)\n",
    "attention_mask = attention_mask.astype(np.int32)\n",
    "token_type_ids = token_type_ids.astype(np.int32)\n",
    "bert_output = np.empty((1, 16, 30522), dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在device上分配内存\n",
    "d_input_ids = cuda.mem_alloc(input_ids.nbytes)\n",
    "d_token_type_ids = cuda.mem_alloc(token_type_ids.nbytes)\n",
    "d_attention_mask = cuda.mem_alloc(attention_mask.nbytes)\n",
    "d_output = cuda.mem_alloc(bert_output.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将内存缓冲区 与 执行上下文中的输入输出张量的地址 相绑定\n",
    "bindings = [int(d_input_ids), int(d_attention_mask),int(d_token_type_ids), int(d_output)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = cuda.Stream()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorrt engine with plan model running time (plus data movement): 0.0030427050001890165\n",
      "****************************************\n",
      "tensorrt engine with plan model running time (without movement): 0.0003354013999341987\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    bert_context.execute_async_v2( bindings, stream.handle, None)\n",
    "start = time.perf_counter()\n",
    "for i in range(10):\n",
    "    # Transfer input data from python buffers to device(GPU)\n",
    "    cuda.memcpy_htod_async(d_input_ids, input_ids, stream)\n",
    "    cuda.memcpy_htod_async(d_token_type_ids, token_type_ids, stream)\n",
    "    cuda.memcpy_htod_async(d_attention_mask, attention_mask, stream)\n",
    "    bert_context.execute_async_v2( bindings, stream.handle, None)\n",
    "    cuda.memcpy_dtoh_async(bert_output, d_output, stream)\n",
    "end = time.perf_counter()\n",
    "stream.synchronize()\n",
    "print(\"tensorrt engine with plan model running time (plus data movement):\", (end-start)*100, \"ms\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "for i in range(10):\n",
    "    bert_context.execute_async_v2( bindings, stream.handle, None)\n",
    "end = time.perf_counter()\n",
    "print('*' * 40)\n",
    "print(\"tensorrt engine with plan model running time (without movement):\", (end-start)*100, \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplify SUCCESS!!!!!!, precision loss:0.1\n"
     ]
    }
   ],
   "source": [
    "required_precission = 1e-1\n",
    "precesion_loss = np.abs(bert_output  - data['logits'])\n",
    "boolean_mask = precesion_loss > required_precission\n",
    "if(len(np.where(boolean_mask)[0]) > 0):\n",
    "    print(\"Simplify ERROR!\")\n",
    "else:\n",
    "    print(f\"Simplify SUCCESS!!!!!!, precision loss:{required_precission}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
