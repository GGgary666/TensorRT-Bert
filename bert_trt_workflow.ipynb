{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import transformers\n",
    "# from tqdm import tqdm\n",
    "import onnxruntime as ort\n",
    "import time\n",
    "import onnx\n",
    "import tensorrt as trt\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 测试 Bert Model\n",
    "1. 初始化tokenizer和Bert model，设置用于测试的text\n",
    "2. 基于pytorch执行bert推理，输出概率最高的10个词\n",
    "3. 保存输出信息，用来和之后转换过的模型进行对比\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "BERT_PATH = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_PATH)\n",
    "model = BertForMaskedLM.from_pretrained(BERT_PATH, return_dict = True)\n",
    "text = \"The capital of France, \" + tokenizer.mask_token + \", contains the Eiffel Tower.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input ids: \n",
      " tensor([[  101,  1996,  3007,  1997,  2605,  1010,   103,  1010,  3397,  1996,\n",
      "          1041, 13355,  2884,  3578,  1012,   102]])\n",
      "output shape:  torch.Size([1, 16, 30522])\n",
      "model test topk10 output:\n",
      "The capital of France, paris, contains the Eiffel Tower.\n",
      "The capital of France, lyon, contains the Eiffel Tower.\n",
      "The capital of France, lille, contains the Eiffel Tower.\n",
      "The capital of France, toulouse, contains the Eiffel Tower.\n",
      "The capital of France, marseille, contains the Eiffel Tower.\n",
      "The capital of France, orleans, contains the Eiffel Tower.\n",
      "The capital of France, strasbourg, contains the Eiffel Tower.\n",
      "The capital of France, nice, contains the Eiffel Tower.\n",
      "The capital of France, cannes, contains the Eiffel Tower.\n",
      "The capital of France, versailles, contains the Eiffel Tower.\n",
      "****************************************\n",
      "pytorch with bin model running time: 0.022047897599986755\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer.encode_plus(text, return_tensors = \"pt\")\n",
    "mask_index = torch.where(encoded_input[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
    "print(\"input ids: \\n\",encoded_input[\"input_ids\"])\n",
    "\n",
    "# warm up\n",
    "for i in range(5):\n",
    "    output = model(**encoded_input)\n",
    "start_time = time.perf_counter()\n",
    "# 计算平均推理时间\n",
    "for i in range(10):\n",
    "    output = model(**encoded_input)\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "print(\"output shape: \", output[0].shape)\n",
    "logits = output.logits\n",
    "softmax = F.softmax(logits, dim = -1)\n",
    "mask_word = softmax[0, mask_index, :]\n",
    "top_10 = torch.topk(mask_word, 10, dim = 1)[1][0]\n",
    "print(\"model test topk10 output:\")\n",
    "for token in top_10:\n",
    "    word = tokenizer.decode([token])\n",
    "    new_sentence = text.replace(tokenizer.mask_token, word)\n",
    "    print(new_sentence)\n",
    "print('*' * 40)\n",
    "print(\"pytorch with bin model running time:\", (end_time-start_time)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['input_ids'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -6.6462,  -6.6775,  -6.6606,  ...,  -5.9660,  -5.7844,  -4.1951],\n",
       "        [-14.7222, -15.2151, -15.0513,  ..., -13.5289, -11.3960, -14.5610],\n",
       "        [-10.1223, -10.7297, -10.1163,  ...,  -9.2822,  -7.6954, -15.4930],\n",
       "        ...,\n",
       "        [-10.7090, -11.2617, -10.9946,  ...,  -8.4995,  -9.6521, -14.2806],\n",
       "        [-12.2987, -12.0131, -12.5270,  ..., -10.8341, -11.2091,  -5.0134],\n",
       "        [-12.7292, -13.4996, -13.1655,  ..., -13.2183, -10.6310, -12.8908]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving inputs and output to case_data.npz ...\n",
      "position id:  tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]],\n",
      "       dtype=torch.int32)\n",
      "input_id shape:  (1, 16)\n",
      "saved input ids: \n",
      " [[  101  1996  3007  1997  2605  1010   103  1010  3397  1996  1041 13355\n",
      "   2884  3578  1012   102]]\n"
     ]
    }
   ],
   "source": [
    "# save inputs and output\n",
    "print(\"Saving inputs and output to case_data.npz ...\")\n",
    "position_ids = torch.arange(0, encoded_input['input_ids'].shape[1]).int().view(1, -1)\n",
    "print(\"position id: \",position_ids)\n",
    "input_ids=encoded_input['input_ids'].int().detach().numpy()\n",
    "token_type_ids=encoded_input['token_type_ids'].int().detach().numpy()\n",
    "print(\"input_id shape: \",input_ids.shape)\n",
    "# save data\n",
    "npz_file = BERT_PATH + '/case_data.npz'\n",
    "np.savez(npz_file,\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            logits=output[0].detach().numpy())\n",
    "\n",
    "data = np.load(npz_file)\n",
    "print(\"saved input ids: \\n\", data['input_ids'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 将模型转换为ONNX格式\n",
    "使用torch.onnx.export() 进行转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported at  bert-base-uncased/model.onnx\n"
     ]
    }
   ],
   "source": [
    "# convert model to onnx\n",
    "model.eval()\n",
    "export_model_path = BERT_PATH + \"/model.onnx\"\n",
    "opset_version = 16\n",
    "symbolic_names = {0: 'batch_size', 1: 'max_seq_len'}\n",
    "torch.onnx.export(  model,                                            \n",
    "                    args=tuple(encoded_input.values()),               # model input (or a tuple for multiple inputs)\n",
    "                    f=export_model_path,                              # where to save the model (can be a file or file-like object)\n",
    "                    opset_version=opset_version,                      # the ONNX version to export the model to\n",
    "                    do_constant_folding=False,                        # whether to execute constant folding for optimization\n",
    "                    input_names=['input_ids',                         # the model's input names\n",
    "                                'attention_mask',\n",
    "                                'token_type_ids'],\n",
    "                    output_names=['logits'],                          # the model's output names\n",
    "                    dynamic_axes={'input_ids': symbolic_names,        # variable length axes\n",
    "                                'attention_mask' : symbolic_names,\n",
    "                                'token_type_ids' : symbolic_names,\n",
    "                                'logits' : symbolic_names})\n",
    "print(\"Model exported at \", export_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 使用onnxruntime进行onnx推理\n",
    "与pytorch和tensorrt的推理时间相对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnxruntime version: 1.16.3\n",
      "onnxruntime device: GPU\n"
     ]
    }
   ],
   "source": [
    "# 检查设备是否为GPU\n",
    "print(\"onnxruntime version:\", ort.__version__)\n",
    "print(\"onnxruntime device:\", ort.get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert SUCCESS!!!!!!\n",
      "****************************************\n",
      "pytorch with bin model running time: 0.02029021099997408\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "session = ort.InferenceSession(export_model_path)\n",
    "# 执行推理\n",
    "# warmup\n",
    "for i in range(5):\n",
    "    outputs = session.run(['logits'], {'input_ids': encoded_input['input_ids'].numpy(),\n",
    "                                    'attention_mask': encoded_input['attention_mask'].numpy(),\n",
    "                                   'token_type_ids': encoded_input['token_type_ids'].numpy()})[0]\n",
    "start_time = time.perf_counter()\n",
    "for i in range(10):\n",
    "    outputs = session.run(['logits'], {'input_ids': encoded_input['input_ids'].numpy(),\n",
    "                                    'attention_mask': encoded_input['attention_mask'].numpy(),\n",
    "                                   'token_type_ids': encoded_input['token_type_ids'].numpy()})[0]\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "# 检查转换后的模型的精度损失情况\n",
    "required_precission = 1e-4\n",
    "precesion_loss = np.abs(outputs - data['logits'])\n",
    "boolean_mask = precesion_loss > required_precission\n",
    "if(len(np.where(boolean_mask)[0]) > 0):\n",
    "    print(\"Convert ERROR!\")\n",
    "else:\n",
    "    print(\"Convert SUCCESS!!!!!!\")\n",
    "print('*' * 40)\n",
    "print(\"pytorch with bin model running time:\", (end_time-start_time)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -6.6461678,  -6.677546 ,  -6.6606207, ...,  -5.966043 ,\n",
       "         -5.7843575,  -4.195074 ],\n",
       "       [-14.722224 , -15.215152 , -15.051269 , ..., -13.528884 ,\n",
       "        -11.39604  , -14.560963 ],\n",
       "       [-10.122324 , -10.729722 , -10.11626  , ...,  -9.282214 ,\n",
       "         -7.695395 , -15.492979 ],\n",
       "       ...,\n",
       "       [-10.708985 , -11.261727 , -10.994601 , ...,  -8.499443 ,\n",
       "         -9.652082 , -14.280533 ],\n",
       "       [-12.298677 , -12.013136 , -12.527017 , ..., -10.834092 ,\n",
       "        -11.209141 ,  -5.0133815],\n",
       "       [-12.729224 , -13.499596 , -13.165469 , ..., -13.21833  ,\n",
       "        -10.630962 , -12.8908415]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['input_ids'].numpy().dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简化ONNX model\n",
    "使用onnxsim库，进行常量折叠。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplifying\u001b[33m...\u001b[0m\n",
      "Finish! Here is the difference:\n",
      "┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOriginal Model\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSimplified Model\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
      "│ Add        │ 177            │ 177              │\n",
      "│ Cast       │ 1              │ 1                │\n",
      "│ Concat     │ 48             │ \u001b[1;32m0               \u001b[0m │\n",
      "│ Constant   │ 596            │ \u001b[1;32m213             \u001b[0m │\n",
      "│ Div        │ 51             │ 51               │\n",
      "│ Erf        │ 13             │ 13               │\n",
      "│ Gather     │ 100            │ \u001b[1;32m2               \u001b[0m │\n",
      "│ MatMul     │ 98             │ 98               │\n",
      "│ Mul        │ 53             │ 53               │\n",
      "│ Pow        │ 26             │ 26               │\n",
      "│ ReduceMean │ 52             │ 52               │\n",
      "│ Reshape    │ 48             │ 48               │\n",
      "│ Shape      │ 97             │ \u001b[1;32m0               \u001b[0m │\n",
      "│ Slice      │ 1              │ \u001b[1;32m0               \u001b[0m │\n",
      "│ Softmax    │ 12             │ 12               │\n",
      "│ Sqrt       │ 26             │ 26               │\n",
      "│ Sub        │ 27             │ 27               │\n",
      "│ Transpose  │ 122            │ \u001b[1;32m48              \u001b[0m │\n",
      "│ Unsqueeze  │ 186            │ \u001b[1;32m1               \u001b[0m │\n",
      "│ Model Size │ 418.1MiB       │ 505.9MiB         │\n",
      "└────────────┴────────────────┴──────────────────┘\n"
     ]
    }
   ],
   "source": [
    "!onnxsim bert-base-uncased/model.onnx bert-base-uncased/model-sim.onnx --overwrite-input-shape input_ids:1,16 token_type_ids:1,16 attention_mask:1,16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplify SUCCESS!!!!!!\n",
      "****************************************\n",
      "pytorch with bin model running time: 0.01595932879990869\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "session = ort.InferenceSession(\"bert-base-uncased/model-sim.onnx\")\n",
    "# 执行推理\n",
    "# warmup\n",
    "for i in range(5):\n",
    "    outputs1 = session.run(['logits'], {'input_ids': encoded_input['input_ids'].numpy(),\n",
    "                                    'attention_mask': encoded_input['attention_mask'].numpy(),\n",
    "                                   'token_type_ids': encoded_input['token_type_ids'].numpy()})[0]\n",
    "start_time = time.perf_counter()\n",
    "for i in range(10):\n",
    "    outputs1 = session.run(['logits'], {'input_ids': encoded_input['input_ids'].numpy(),\n",
    "                                    'attention_mask': encoded_input['attention_mask'].numpy(),\n",
    "                                   'token_type_ids': encoded_input['token_type_ids'].numpy()})[0]\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "# 检查转换后的模型的精度损失情况\n",
    "required_precission = 1e-4\n",
    "precesion_loss = np.abs(outputs1 - data['logits'])\n",
    "boolean_mask = precesion_loss > required_precission\n",
    "if(len(np.where(boolean_mask)[0]) > 0):\n",
    "    print(\"Simplify ERROR!\")\n",
    "else:\n",
    "    print(\"Simplify SUCCESS!!!!!!\")\n",
    "print('*' * 40)\n",
    "print(\"pytorch with bin model running time:\", (end_time-start_time)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building engine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1540/1890754784.py:9: DeprecationWarning: Use set_memory_pool_limit instead.\n",
      "  config.max_workspace_size = max_ws\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/02/2024-13:01:10] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "network.num_layers 1162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1540/1890754784.py:19: DeprecationWarning: Use build_serialized_network instead.\n",
      "  engine = builder.build_engine(network, config=config)\n"
     ]
    }
   ],
   "source": [
    "# def build_engine(model_file, max_ws=512*1024*1024, fp16=False):\n",
    "#     print(\"building engine\")\n",
    "#     TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "#     builder = trt.Builder(TRT_LOGGER)\n",
    "\n",
    "#     config = builder.create_builder_config()\n",
    "#     if fp16:\n",
    "#         config.set_flag(trt.BuilderFlag.FP16)\n",
    "#     config.max_workspace_size = max_ws\n",
    "    \n",
    "#     explicit_batch = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "#     network = builder.create_network(explicit_batch)\n",
    "#     with trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "#         with open(model_file, 'rb') as model:\n",
    "#             parsed = parser.parse(model.read())\n",
    "#             print(\"network.num_layers\", network.num_layers)\n",
    "#             #last_layer = network.get_layer(network.num_layers - 1)\n",
    "#             #network.mark_output(last_layer.get_output(0))\n",
    "#             engine = builder.build_engine(network, config=config)\n",
    "#             return engine\n",
    "            \n",
    "# engine = build_engine(\"bert-base-uncased/model-sim.onnx\")\n",
    "# # save the paln model\n",
    "# BERT_PATH = 'bert-base-uncased'\n",
    "# plan_path = BERT_PATH +'/model.plan'\n",
    "# with open(plan_path, 'wb') as f:\n",
    "#     f.write(engine.serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_174111/1623577434.py:8: DeprecationWarning: Use set_memory_pool_limit instead.\n",
      "  config.max_workspace_size = 512*1024*1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/02/2024-17:12:40] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "network.num_layers 1162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_174111/1623577434.py:18: DeprecationWarning: Use build_serialized_network instead.\n",
      "  engine = builder.build_engine(network, config=config)\n"
     ]
    }
   ],
   "source": [
    "# 创建engine\n",
    "model_file = \"bert-base-uncased/model-sim.onnx\"\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "builder = trt.Builder(TRT_LOGGER)\n",
    "\n",
    "config = builder.create_builder_config()\n",
    "\n",
    "config.max_workspace_size = 512*1024*1024\n",
    "\n",
    "explicit_batch = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "network = builder.create_network(explicit_batch)\n",
    "with trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "    with open(model_file, 'rb') as model:\n",
    "        parsed = parser.parse(model.read())\n",
    "        print(\"network.num_layers\", network.num_layers)\n",
    "        #last_layer = network.get_layer(network.num_layers - 1)\n",
    "        #network.mark_output(last_layer.get_output(0))\n",
    "        engine = builder.build_engine(network, config=config)\n",
    "        \n",
    "        \n",
    "# save the paln model\n",
    "# BERT_PATH = 'bert-base-uncased'\n",
    "# plan_path = BERT_PATH +'/model.plan'\n",
    "# with open(plan_path, 'wb') as f:\n",
    "#     f.write(engine.serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input id: 0    is input:  TensorIOMode.INPUT   binding name: input_ids   shape: (1, 16) type:  DataType.INT32\n",
      "input id: 1    is input:  TensorIOMode.INPUT   binding name: attention_mask   shape: (1, 16) type:  DataType.INT32\n",
      "input id: 2    is input:  TensorIOMode.INPUT   binding name: token_type_ids   shape: (1, 16) type:  DataType.INT32\n",
      "input id: 3    is input:  TensorIOMode.OUTPUT   binding name: logits   shape: (1, 16, 30522) type:  DataType.FLOAT\n"
     ]
    }
   ],
   "source": [
    "for idx in range(engine.num_bindings):\n",
    "    name = engine.get_tensor_name (idx)\n",
    "    is_input = engine.get_tensor_mode (name)\n",
    "    op_type = engine.get_tensor_dtype(name)\n",
    "    shape = engine.get_tensor_shape(name)\n",
    "\n",
    "    print('input id:',idx,'   is input: ', is_input,'  binding name:', name, '  shape:', shape, 'type: ', op_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8601] # trtexec --onnx=bert-base-uncased/model-sim.onnx --saveEngine=bert-base-uncased/model.trt --explicitBatch\n",
      "[02/02/2024-13:05:32] [W] --explicitBatch flag has been deprecated and has no effect!\n",
      "[02/02/2024-13:05:32] [W] Explicit batch dim is automatically enabled if input model is ONNX or if dynamic shapes are provided when the engine is built.\n",
      "[02/02/2024-13:05:32] [I] === Model Options ===\n",
      "[02/02/2024-13:05:32] [I] Format: ONNX\n",
      "[02/02/2024-13:05:32] [I] Model: bert-base-uncased/model-sim.onnx\n",
      "[02/02/2024-13:05:32] [I] Output:\n",
      "[02/02/2024-13:05:32] [I] === Build Options ===\n",
      "[02/02/2024-13:05:32] [I] Max batch: explicit batch\n",
      "[02/02/2024-13:05:32] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default\n",
      "[02/02/2024-13:05:32] [I] minTiming: 1\n",
      "[02/02/2024-13:05:32] [I] avgTiming: 8\n",
      "[02/02/2024-13:05:32] [I] Precision: FP32\n",
      "[02/02/2024-13:05:32] [I] LayerPrecisions: \n",
      "[02/02/2024-13:05:32] [I] Layer Device Types: \n",
      "[02/02/2024-13:05:32] [I] Calibration: \n",
      "[02/02/2024-13:05:32] [I] Refit: Disabled\n",
      "[02/02/2024-13:05:32] [I] Version Compatible: Disabled\n",
      "[02/02/2024-13:05:32] [I] TensorRT runtime: full\n",
      "[02/02/2024-13:05:32] [I] Lean DLL Path: \n",
      "[02/02/2024-13:05:32] [I] Tempfile Controls: { in_memory: allow, temporary: allow }\n",
      "[02/02/2024-13:05:32] [I] Exclude Lean Runtime: Disabled\n",
      "[02/02/2024-13:05:32] [I] Sparsity: Disabled\n",
      "[02/02/2024-13:05:32] [I] Safe mode: Disabled\n",
      "[02/02/2024-13:05:32] [I] Build DLA standalone loadable: Disabled\n",
      "[02/02/2024-13:05:32] [I] Allow GPU fallback for DLA: Disabled\n",
      "[02/02/2024-13:05:32] [I] DirectIO mode: Disabled\n",
      "[02/02/2024-13:05:32] [I] Restricted mode: Disabled\n",
      "[02/02/2024-13:05:32] [I] Skip inference: Disabled\n",
      "[02/02/2024-13:05:32] [I] Save engine: bert-base-uncased/model.trt\n",
      "[02/02/2024-13:05:32] [I] Load engine: \n",
      "[02/02/2024-13:05:32] [I] Profiling verbosity: 0\n",
      "[02/02/2024-13:05:32] [I] Tactic sources: Using default tactic sources\n",
      "[02/02/2024-13:05:32] [I] timingCacheMode: local\n",
      "[02/02/2024-13:05:32] [I] timingCacheFile: \n",
      "[02/02/2024-13:05:32] [I] Heuristic: Disabled\n",
      "[02/02/2024-13:05:32] [I] Preview Features: Use default preview flags.\n",
      "[02/02/2024-13:05:32] [I] MaxAuxStreams: -1\n",
      "[02/02/2024-13:05:32] [I] BuilderOptimizationLevel: -1\n",
      "[02/02/2024-13:05:32] [I] Input(s)s format: fp32:CHW\n",
      "[02/02/2024-13:05:32] [I] Output(s)s format: fp32:CHW\n",
      "[02/02/2024-13:05:32] [I] Input build shapes: model\n",
      "[02/02/2024-13:05:32] [I] Input calibration shapes: model\n",
      "[02/02/2024-13:05:32] [I] === System Options ===\n",
      "[02/02/2024-13:05:32] [I] Device: 0\n",
      "[02/02/2024-13:05:32] [I] DLACore: \n",
      "[02/02/2024-13:05:32] [I] Plugins:\n",
      "[02/02/2024-13:05:32] [I] setPluginsToSerialize:\n",
      "[02/02/2024-13:05:32] [I] dynamicPlugins:\n",
      "[02/02/2024-13:05:32] [I] ignoreParsedPluginLibs: 0\n",
      "[02/02/2024-13:05:32] [I] \n",
      "[02/02/2024-13:05:32] [I] === Inference Options ===\n",
      "[02/02/2024-13:05:32] [I] Batch: Explicit\n",
      "[02/02/2024-13:05:32] [I] Input inference shapes: model\n",
      "[02/02/2024-13:05:32] [I] Iterations: 10\n",
      "[02/02/2024-13:05:32] [I] Duration: 3s (+ 200ms warm up)\n",
      "[02/02/2024-13:05:32] [I] Sleep time: 0ms\n",
      "[02/02/2024-13:05:32] [I] Idle time: 0ms\n",
      "[02/02/2024-13:05:32] [I] Inference Streams: 1\n",
      "[02/02/2024-13:05:32] [I] ExposeDMA: Disabled\n",
      "[02/02/2024-13:05:32] [I] Data transfers: Enabled\n",
      "[02/02/2024-13:05:32] [I] Spin-wait: Disabled\n",
      "[02/02/2024-13:05:32] [I] Multithreading: Disabled\n",
      "[02/02/2024-13:05:32] [I] CUDA Graph: Disabled\n",
      "[02/02/2024-13:05:32] [I] Separate profiling: Disabled\n",
      "[02/02/2024-13:05:32] [I] Time Deserialize: Disabled\n",
      "[02/02/2024-13:05:32] [I] Time Refit: Disabled\n",
      "[02/02/2024-13:05:32] [I] NVTX verbosity: 0\n",
      "[02/02/2024-13:05:32] [I] Persistent Cache Ratio: 0\n",
      "[02/02/2024-13:05:32] [I] Inputs:\n",
      "[02/02/2024-13:05:32] [I] === Reporting Options ===\n",
      "[02/02/2024-13:05:32] [I] Verbose: Disabled\n",
      "[02/02/2024-13:05:32] [I] Averages: 10 inferences\n",
      "[02/02/2024-13:05:32] [I] Percentiles: 90,95,99\n",
      "[02/02/2024-13:05:32] [I] Dump refittable layers:Disabled\n",
      "[02/02/2024-13:05:32] [I] Dump output: Disabled\n",
      "[02/02/2024-13:05:32] [I] Profile: Disabled\n",
      "[02/02/2024-13:05:32] [I] Export timing to JSON file: \n",
      "[02/02/2024-13:05:32] [I] Export output to JSON file: \n",
      "[02/02/2024-13:05:32] [I] Export profile to JSON file: \n",
      "[02/02/2024-13:05:32] [I] \n",
      "[02/02/2024-13:05:32] [I] === Device Information ===\n",
      "[02/02/2024-13:05:32] [I] Selected Device: NVIDIA GeForce RTX 4060 Ti\n",
      "[02/02/2024-13:05:32] [I] Compute Capability: 8.9\n",
      "[02/02/2024-13:05:32] [I] SMs: 34\n",
      "[02/02/2024-13:05:32] [I] Device Global Memory: 16074 MiB\n",
      "[02/02/2024-13:05:32] [I] Shared Memory per SM: 100 KiB\n",
      "[02/02/2024-13:05:32] [I] Memory Bus Width: 128 bits (ECC disabled)\n",
      "[02/02/2024-13:05:32] [I] Application Compute Clock Rate: 2.565 GHz\n",
      "[02/02/2024-13:05:32] [I] Application Memory Clock Rate: 9.001 GHz\n",
      "[02/02/2024-13:05:32] [I] \n",
      "[02/02/2024-13:05:32] [I] Note: The application clock rates do not reflect the actual clock rates that the GPU is currently running at.\n",
      "[02/02/2024-13:05:32] [I] \n",
      "[02/02/2024-13:05:32] [I] TensorRT version: 8.6.1\n",
      "[02/02/2024-13:05:32] [I] Loading standard plugins\n",
      "[02/02/2024-13:05:32] [I] [TRT] [MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 19, GPU 1034 (MiB)\n",
      "[02/02/2024-13:05:36] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +1445, GPU +268, now: CPU 1540, GPU 1304 (MiB)\n",
      "[02/02/2024-13:05:36] [I] Start parsing network model.\n",
      "[02/02/2024-13:05:36] [I] [TRT] ----------------------------------------------------------------\n",
      "[02/02/2024-13:05:36] [I] [TRT] Input filename:   bert-base-uncased/model-sim.onnx\n",
      "[02/02/2024-13:05:36] [I] [TRT] ONNX IR version:  0.0.8\n",
      "[02/02/2024-13:05:36] [I] [TRT] Opset version:    16\n",
      "[02/02/2024-13:05:36] [I] [TRT] Producer name:    pytorch\n",
      "[02/02/2024-13:05:36] [I] [TRT] Producer version: 2.2.0\n",
      "[02/02/2024-13:05:36] [I] [TRT] Domain:           \n",
      "[02/02/2024-13:05:36] [I] [TRT] Model version:    0\n",
      "[02/02/2024-13:05:36] [I] [TRT] Doc string:       \n",
      "[02/02/2024-13:05:36] [I] [TRT] ----------------------------------------------------------------\n",
      "[02/02/2024-13:05:37] [W] [TRT] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[02/02/2024-13:05:37] [I] Finished parsing network model. Parse time: 0.685949\n",
      "[02/02/2024-13:05:37] [I] [TRT] Graph optimization time: 0.0390756 seconds.\n",
      "[02/02/2024-13:05:37] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[02/02/2024-13:05:55] [I] [TRT] Detected 3 inputs and 1 output network tensors.\n",
      "[02/02/2024-13:05:55] [I] [TRT] Total Host Persistent Memory: 32\n",
      "[02/02/2024-13:05:55] [I] [TRT] Total Device Persistent Memory: 0\n",
      "[02/02/2024-13:05:55] [I] [TRT] Total Scratch Memory: 664064\n",
      "[02/02/2024-13:05:55] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 509 MiB\n",
      "[02/02/2024-13:05:55] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 1 steps to complete.\n",
      "[02/02/2024-13:05:55] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 0.006457ms to assign 1 blocks to 1 nodes requiring 664064 bytes.\n",
      "[02/02/2024-13:05:55] [I] [TRT] Total Activation Memory: 664064\n",
      "[02/02/2024-13:05:56] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +512, now: CPU 0, GPU 512 (MiB)\n",
      "[02/02/2024-13:05:56] [I] Engine built in 24.4592 sec.\n",
      "[02/02/2024-13:05:57] [I] [TRT] Loaded engine size: 506 MiB\n",
      "[02/02/2024-13:05:57] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +505, now: CPU 0, GPU 505 (MiB)\n",
      "[02/02/2024-13:05:57] [I] Engine deserialized in 0.284354 sec.\n",
      "[02/02/2024-13:05:57] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +1, now: CPU 0, GPU 506 (MiB)\n",
      "[02/02/2024-13:05:57] [I] Setting persistentCacheLimit to 0 bytes.\n",
      "[02/02/2024-13:05:57] [I] Using random values for input input_ids\n",
      "[02/02/2024-13:05:57] [I] Input binding for input_ids with dimensions 1x16 is created.\n",
      "[02/02/2024-13:05:57] [I] Using random values for input attention_mask\n",
      "[02/02/2024-13:05:57] [I] Input binding for attention_mask with dimensions 1x16 is created.\n",
      "[02/02/2024-13:05:57] [I] Using random values for input token_type_ids\n",
      "[02/02/2024-13:05:57] [I] Input binding for token_type_ids with dimensions 1x16 is created.\n",
      "[02/02/2024-13:05:57] [I] Output binding for logits with dimensions 1x16x30522 is created.\n",
      "[02/02/2024-13:05:57] [I] Starting inference\n",
      "[02/02/2024-13:06:00] [I] Warmup completed 89 queries over 200 ms\n",
      "[02/02/2024-13:06:00] [I] Timing trace has 1362 queries over 3.00613 s\n",
      "[02/02/2024-13:06:00] [I] \n",
      "[02/02/2024-13:06:00] [I] === Trace details ===\n",
      "[02/02/2024-13:06:00] [I] Trace averages of 10 runs:\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.22853 ms - Host latency: 2.41094 ms (enqueue 0.586336 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.2829 ms - Host latency: 2.45987 ms (enqueue 0.720136 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.13637 ms - Host latency: 2.3129 ms (enqueue 0.6231 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21136 ms - Host latency: 2.39043 ms (enqueue 0.75672 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21998 ms - Host latency: 2.43589 ms (enqueue 0.734338 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.19361 ms - Host latency: 2.36693 ms (enqueue 0.678088 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.22338 ms - Host latency: 2.40355 ms (enqueue 0.73876 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.28179 ms - Host latency: 2.46013 ms (enqueue 0.69592 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17365 ms - Host latency: 2.35204 ms (enqueue 0.740778 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.24759 ms - Host latency: 2.42375 ms (enqueue 0.704315 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.32014 ms - Host latency: 2.5006 ms (enqueue 0.781061 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17437 ms - Host latency: 2.37447 ms (enqueue 0.734659 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.23723 ms - Host latency: 2.41367 ms (enqueue 0.701706 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.28332 ms - Host latency: 2.45455 ms (enqueue 0.592255 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17651 ms - Host latency: 2.34794 ms (enqueue 0.649884 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20099 ms - Host latency: 2.39803 ms (enqueue 0.663196 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.23487 ms - Host latency: 2.41335 ms (enqueue 0.695416 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.15954 ms - Host latency: 2.33643 ms (enqueue 0.746429 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21359 ms - Host latency: 2.42303 ms (enqueue 0.692944 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.27914 ms - Host latency: 2.4514 ms (enqueue 0.652405 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.12941 ms - Host latency: 2.30851 ms (enqueue 0.39007 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21285 ms - Host latency: 2.38579 ms (enqueue 0.327277 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17969 ms - Host latency: 2.34836 ms (enqueue 0.378314 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.23571 ms - Host latency: 2.40016 ms (enqueue 0.222876 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18879 ms - Host latency: 2.35286 ms (enqueue 0.262231 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18082 ms - Host latency: 2.34791 ms (enqueue 0.380676 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20909 ms - Host latency: 2.37885 ms (enqueue 0.548114 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.16677 ms - Host latency: 2.33036 ms (enqueue 0.320453 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18492 ms - Host latency: 2.3545 ms (enqueue 0.634058 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17118 ms - Host latency: 2.34364 ms (enqueue 0.629175 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.24861 ms - Host latency: 2.42137 ms (enqueue 0.644641 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17826 ms - Host latency: 2.3504 ms (enqueue 0.674048 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20518 ms - Host latency: 2.38011 ms (enqueue 0.692413 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.2399 ms - Host latency: 2.41335 ms (enqueue 0.665417 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18585 ms - Host latency: 2.39392 ms (enqueue 0.618805 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21503 ms - Host latency: 2.3795 ms (enqueue 0.34422 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.23058 ms - Host latency: 2.46426 ms (enqueue 0.603632 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18572 ms - Host latency: 2.35602 ms (enqueue 0.661157 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18962 ms - Host latency: 2.44231 ms (enqueue 0.597986 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.24384 ms - Host latency: 2.4182 ms (enqueue 0.492236 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17313 ms - Host latency: 2.35718 ms (enqueue 0.547034 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.19945 ms - Host latency: 2.37567 ms (enqueue 0.728394 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20366 ms - Host latency: 2.38842 ms (enqueue 0.754541 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.1639 ms - Host latency: 2.34023 ms (enqueue 0.720276 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.19907 ms - Host latency: 2.37306 ms (enqueue 0.679785 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.25496 ms - Host latency: 2.43036 ms (enqueue 0.71991 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.1653 ms - Host latency: 2.34224 ms (enqueue 0.654175 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.28433 ms - Host latency: 2.48632 ms (enqueue 0.613135 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.25422 ms - Host latency: 2.42512 ms (enqueue 0.634595 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.1627 ms - Host latency: 2.38567 ms (enqueue 0.691284 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.22422 ms - Host latency: 2.3978 ms (enqueue 0.681067 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18246 ms - Host latency: 2.3551 ms (enqueue 0.737451 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18678 ms - Host latency: 2.36085 ms (enqueue 0.700293 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.2309 ms - Host latency: 2.40273 ms (enqueue 0.687244 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.1939 ms - Host latency: 2.36589 ms (enqueue 0.682983 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.1849 ms - Host latency: 2.35665 ms (enqueue 0.640588 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20557 ms - Host latency: 2.40814 ms (enqueue 0.690649 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.19557 ms - Host latency: 2.37277 ms (enqueue 0.671826 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18929 ms - Host latency: 2.39972 ms (enqueue 0.686462 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.25649 ms - Host latency: 2.42864 ms (enqueue 0.650879 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17297 ms - Host latency: 2.34592 ms (enqueue 0.66366 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.22762 ms - Host latency: 2.40157 ms (enqueue 0.68866 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.26277 ms - Host latency: 2.4761 ms (enqueue 0.667993 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.1651 ms - Host latency: 2.33254 ms (enqueue 0.531006 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.25006 ms - Host latency: 2.42709 ms (enqueue 0.645215 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.23591 ms - Host latency: 2.44139 ms (enqueue 0.54353 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18702 ms - Host latency: 2.36449 ms (enqueue 0.665039 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.16669 ms - Host latency: 2.3573 ms (enqueue 0.4479 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.25352 ms - Host latency: 2.42657 ms (enqueue 0.530225 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.14072 ms - Host latency: 2.31387 ms (enqueue 0.640271 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20887 ms - Host latency: 2.45624 ms (enqueue 0.801306 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21603 ms - Host latency: 2.39735 ms (enqueue 0.741895 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.16801 ms - Host latency: 2.35758 ms (enqueue 0.402319 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20116 ms - Host latency: 2.36769 ms (enqueue 0.443982 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20703 ms - Host latency: 2.43057 ms (enqueue 0.440149 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.19373 ms - Host latency: 2.37385 ms (enqueue 0.592114 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20662 ms - Host latency: 2.38062 ms (enqueue 0.422168 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18929 ms - Host latency: 2.35677 ms (enqueue 0.324292 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.12858 ms - Host latency: 2.28925 ms (enqueue 0.168359 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.19937 ms - Host latency: 2.36097 ms (enqueue 0.248633 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.15195 ms - Host latency: 2.32847 ms (enqueue 0.52251 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17885 ms - Host latency: 2.34467 ms (enqueue 0.437256 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.24469 ms - Host latency: 2.41315 ms (enqueue 0.513818 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.13184 ms - Host latency: 2.30193 ms (enqueue 0.635059 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.22476 ms - Host latency: 2.39463 ms (enqueue 0.632471 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.26194 ms - Host latency: 2.5146 ms (enqueue 0.63938 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17175 ms - Host latency: 2.34399 ms (enqueue 0.658716 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20942 ms - Host latency: 2.47114 ms (enqueue 0.778491 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21616 ms - Host latency: 2.39783 ms (enqueue 0.596338 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.1252 ms - Host latency: 2.3073 ms (enqueue 0.567627 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20569 ms - Host latency: 2.37808 ms (enqueue 0.434839 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18909 ms - Host latency: 2.3575 ms (enqueue 0.594775 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18413 ms - Host latency: 2.38032 ms (enqueue 0.634814 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.26125 ms - Host latency: 2.43289 ms (enqueue 0.655981 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.19653 ms - Host latency: 2.38064 ms (enqueue 0.62627 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20635 ms - Host latency: 2.37617 ms (enqueue 0.649878 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21355 ms - Host latency: 2.40034 ms (enqueue 0.618555 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.26321 ms - Host latency: 2.43354 ms (enqueue 0.605054 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17314 ms - Host latency: 2.34272 ms (enqueue 0.654834 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.23301 ms - Host latency: 2.39905 ms (enqueue 0.43147 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21267 ms - Host latency: 2.37673 ms (enqueue 0.427051 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.13328 ms - Host latency: 2.29724 ms (enqueue 0.248267 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20906 ms - Host latency: 2.37507 ms (enqueue 0.331616 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.14387 ms - Host latency: 2.3386 ms (enqueue 0.430688 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.22371 ms - Host latency: 2.3999 ms (enqueue 0.653882 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.26194 ms - Host latency: 2.43025 ms (enqueue 0.548853 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.15469 ms - Host latency: 2.32393 ms (enqueue 0.52439 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21699 ms - Host latency: 2.4072 ms (enqueue 0.643384 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.24517 ms - Host latency: 2.41895 ms (enqueue 0.680054 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.13586 ms - Host latency: 2.30842 ms (enqueue 0.647192 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.22661 ms - Host latency: 2.39636 ms (enqueue 0.540503 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.34346 ms - Host latency: 2.5134 ms (enqueue 0.459595 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.13032 ms - Host latency: 2.35818 ms (enqueue 0.621362 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.20664 ms - Host latency: 2.3769 ms (enqueue 0.580249 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21753 ms - Host latency: 2.38826 ms (enqueue 0.592358 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18115 ms - Host latency: 2.3501 ms (enqueue 0.617554 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.1782 ms - Host latency: 2.36582 ms (enqueue 0.540088 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21553 ms - Host latency: 2.3927 ms (enqueue 0.677148 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.14443 ms - Host latency: 2.31707 ms (enqueue 0.642163 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21648 ms - Host latency: 2.42805 ms (enqueue 0.603687 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21033 ms - Host latency: 2.37976 ms (enqueue 0.589429 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.16008 ms - Host latency: 2.33853 ms (enqueue 0.670825 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.23396 ms - Host latency: 2.40398 ms (enqueue 0.582642 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.15686 ms - Host latency: 2.32275 ms (enqueue 0.387085 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17585 ms - Host latency: 2.34417 ms (enqueue 0.553809 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.24316 ms - Host latency: 2.41995 ms (enqueue 0.721509 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.15613 ms - Host latency: 2.32949 ms (enqueue 0.697827 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.1884 ms - Host latency: 2.3606 ms (enqueue 0.668823 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.23374 ms - Host latency: 2.40718 ms (enqueue 0.672949 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.15151 ms - Host latency: 2.32485 ms (enqueue 0.702393 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.21355 ms - Host latency: 2.38901 ms (enqueue 0.731299 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.23176 ms - Host latency: 2.40898 ms (enqueue 0.680127 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.16987 ms - Host latency: 2.34687 ms (enqueue 0.718799 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.22839 ms - Host latency: 2.40422 ms (enqueue 0.618066 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.18457 ms - Host latency: 2.35193 ms (enqueue 0.437183 ms)\n",
      "[02/02/2024-13:06:00] [I] Average on 10 runs - GPU latency: 2.17297 ms - Host latency: 2.33674 ms (enqueue 0.228491 ms)\n",
      "[02/02/2024-13:06:00] [I] \n",
      "[02/02/2024-13:06:00] [I] === Performance summary ===\n",
      "[02/02/2024-13:06:00] [I] Throughput: 453.074 qps\n",
      "[02/02/2024-13:06:00] [I] Latency: min = 2.12183 ms, max = 3.22327 ms, mean = 2.38405 ms, median = 2.41956 ms, percentile(90%) = 2.67151 ms, percentile(95%) = 2.71201 ms, percentile(99%) = 2.88208 ms\n",
      "[02/02/2024-13:06:00] [I] Enqueue Time: min = 0.139404 ms, max = 1.23145 ms, mean = 0.592865 ms, median = 0.631531 ms, percentile(90%) = 0.755676 ms, percentile(95%) = 0.852783 ms, percentile(99%) = 1.07861 ms\n",
      "[02/02/2024-13:06:00] [I] H2D Latency: min = 0.00512695 ms, max = 0.504395 ms, mean = 0.0272168 ms, median = 0.0170898 ms, percentile(90%) = 0.034668 ms, percentile(95%) = 0.0465088 ms, percentile(99%) = 0.346436 ms\n",
      "[02/02/2024-13:06:00] [I] GPU Compute Time: min = 1.95874 ms, max = 2.73407 ms, mean = 2.20366 ms, median = 2.24164 ms, percentile(90%) = 2.47607 ms, percentile(95%) = 2.52515 ms, percentile(99%) = 2.59993 ms\n",
      "[02/02/2024-13:06:00] [I] D2H Latency: min = 0.150818 ms, max = 0.164307 ms, mean = 0.153176 ms, median = 0.152893 ms, percentile(90%) = 0.154541 ms, percentile(95%) = 0.155273 ms, percentile(99%) = 0.156372 ms\n",
      "[02/02/2024-13:06:00] [I] Total Host Walltime: 3.00613 s\n",
      "[02/02/2024-13:06:00] [I] Total GPU Compute Time: 3.00138 s\n",
      "[02/02/2024-13:06:00] [W] * GPU compute time is unstable, with coefficient of variance = 9.6148%.\n",
      "[02/02/2024-13:06:00] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.\n",
      "[02/02/2024-13:06:00] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[02/02/2024-13:06:00] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8601] # trtexec --onnx=bert-base-uncased/model-sim.onnx --saveEngine=bert-base-uncased/model.trt --explicitBatch\n"
     ]
    }
   ],
   "source": [
    "# !trtexec --onnx=bert-base-uncased/model-sim.onnx --saveEngine=bert-base-uncased/model.trt  --explicitBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT_PATH = 'bert-base-uncased'\n",
    "# plan_path = BERT_PATH +'/model.plan'\n",
    "\n",
    "# TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "# runtime = trt.Runtime(TRT_LOGGER)\n",
    "# with open(plan_path, 'rb') as f:\n",
    "#     engine_bytes = f.read()\n",
    "#     engine = runtime.deserialize_cuda_engine(engine_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_context = engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=encoded_input['input_ids'].numpy().astype(np.int32)\n",
    "attention_mask = encoded_input['attention_mask'].numpy().astype(np.int32)\n",
    "token_type_ids = encoded_input['token_type_ids'].numpy().astype(np.int32)\n",
    "bert_output = np.empty((1, 16, 30522), dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1996,  3007,  1997,  2605,  1010,   103,  1010,  3397,  1996,\n",
      "          1041, 13355,  2884,  3578,  1012,   102]])\n",
      "[[  101  1996  3007  1997  2605  1010   103  1010  3397  1996  1041 13355\n",
      "   2884  3578  1012   102]]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_input['input_ids'])\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32 (1, 16)\n",
      "int32 (1, 16)\n",
      "int32 (1, 16)\n",
      "float32 (1, 16, 30522)\n"
     ]
    }
   ],
   "source": [
    "print(input_ids.dtype, input_ids.shape)\n",
    "print(attention_mask.dtype, attention_mask.shape)\n",
    "print(token_type_ids.dtype, token_type_ids.shape)\n",
    "print(bert_output.dtype, bert_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d_input_ids = cuda.mem_alloc(input_ids.nbytes)\n",
    "d_token_type_ids = cuda.mem_alloc(token_type_ids.nbytes)\n",
    "d_attention_mask = cuda.mem_alloc(attention_mask.nbytes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_output = cuda.mem_alloc(bert_output.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bindings = [int(d_input_ids), int(d_attention_mask),int(d_token_type_ids), int(d_output)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = cuda.Stream()\n",
    "# Transfer input data from python buffers to device(GPU)\n",
    "cuda.memcpy_htod_async(d_input_ids, input_ids, stream)\n",
    "cuda.memcpy_htod_async(d_token_type_ids, token_type_ids, stream)\n",
    "cuda.memcpy_htod_async(d_attention_mask, attention_mask, stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0018574402800004464\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    bert_context.execute_async_v2( bindings, stream.handle, None)\n",
    "start = time.perf_counter()\n",
    "for i in range(100):\n",
    "    bert_context.execute_async_v2( bindings, stream.handle, None)\n",
    "end = time.perf_counter()\n",
    "print((end-start)/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda.memcpy_dtoh_async(bert_output, d_output, stream)\n",
    "stream.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model test topk10 output:\n",
      "The capital of France, paris, contains the Eiffel Tower.\n",
      "The capital of France, lyon, contains the Eiffel Tower.\n",
      "The capital of France, lille, contains the Eiffel Tower.\n",
      "The capital of France, toulouse, contains the Eiffel Tower.\n",
      "The capital of France, marseille, contains the Eiffel Tower.\n",
      "The capital of France, orleans, contains the Eiffel Tower.\n",
      "The capital of France, strasbourg, contains the Eiffel Tower.\n",
      "The capital of France, nice, contains the Eiffel Tower.\n",
      "The capital of France, cannes, contains the Eiffel Tower.\n",
      "The capital of France, versailles, contains the Eiffel Tower.\n"
     ]
    }
   ],
   "source": [
    "pred = torch.tensor(bert_output)\n",
    "softmax = F.softmax(pred, dim = -1)\n",
    "mask_word = softmax[0, mask_index, :]\n",
    "top_10 = torch.topk(mask_word, 10, dim = 1)[1][0]\n",
    "print(\"model test topk10 output:\")\n",
    "for token in top_10:\n",
    "    word = tokenizer.decode([token])\n",
    "    new_sentence = text.replace(tokenizer.mask_token, word)\n",
    "    print(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -6.6460,  -6.6773,  -6.6604,  ...,  -5.9658,  -5.7841,  -4.1949],\n",
       "         [-14.7249, -15.2179, -15.0539,  ..., -13.5311, -11.3981, -14.5633],\n",
       "         [-10.1212, -10.7288, -10.1155,  ...,  -9.2821,  -7.6942, -15.4952],\n",
       "         ...,\n",
       "         [-10.7082, -11.2609, -10.9939,  ...,  -8.4998,  -9.6510, -14.2810],\n",
       "         [-12.2971, -12.0116, -12.5240,  ..., -10.8314, -11.2067,  -5.0205],\n",
       "         [-12.7315, -13.5021, -13.1680,  ..., -13.2217, -10.6320, -12.8934]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplify SUCCESS!!!!!!\n"
     ]
    }
   ],
   "source": [
    "required_precission = 1e-1\n",
    "precesion_loss = np.abs(pred  - data['logits'])\n",
    "boolean_mask = precesion_loss > required_precission\n",
    "\n",
    "np.where(boolean_mask)[0] > 0\n",
    "if(len(np.where(boolean_mask)[0]) > 0):\n",
    "    print(\"Simplify ERROR!\")\n",
    "else:\n",
    "    print(\"Simplify SUCCESS!!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
